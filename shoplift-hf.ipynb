{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11017899,"sourceType":"datasetVersion","datasetId":6860306}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport time\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport tensorflow as tf\nimport concurrent.futures\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score\nfrom tensorflow.keras import layers, models\nfrom torch.optim.lr_scheduler import StepLR\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:10.149737Z","iopub.execute_input":"2025-03-21T09:06:10.150051Z","iopub.status.idle":"2025-03-21T09:06:17.595967Z","shell.execute_reply.started":"2025-03-21T09:06:10.150017Z","shell.execute_reply":"2025-03-21T09:06:17.595226Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def get_unique_videos(folder_path, underscore_count):\n    video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n    \n    print(f\"\\nTotal videos in '{folder_path}' before filtering: {len(video_files)}\")\n\n    # Filter out videos where the filename contains the specified number of underscores\n    unique_videos = [os.path.join(folder_path, f) for f in video_files if f.count('_') != underscore_count]\n\n    print(f\"Total videos in '{folder_path}' after filtering: {len(unique_videos)}\")\n\n    return unique_videos","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:17.596752Z","iopub.execute_input":"2025-03-21T09:06:17.597377Z","iopub.status.idle":"2025-03-21T09:06:17.601974Z","shell.execute_reply.started":"2025-03-21T09:06:17.597351Z","shell.execute_reply":"2025-03-21T09:06:17.600994Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"non_shoplifters = \"/kaggle/input/shoplift/Shop DataSet/non shop lifters\"\nshoplifters = \"/kaggle/input/shoplift/Shop DataSet/shop lifters\"\n\nnon_shop_lifters_videos = get_unique_videos(non_shoplifters, 4)  \nshop_lifters_videos = get_unique_videos(shoplifters, 3) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:17.602880Z","iopub.execute_input":"2025-03-21T09:06:17.603080Z","iopub.status.idle":"2025-03-21T09:06:17.623168Z","shell.execute_reply.started":"2025-03-21T09:06:17.603062Z","shell.execute_reply":"2025-03-21T09:06:17.622496Z"}},"outputs":[{"name":"stdout","text":"\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/non shop lifters' before filtering: 531\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/non shop lifters' after filtering: 313\n\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/shop lifters' before filtering: 324\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/shop lifters' after filtering: 324\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/shoplift/Shop DataSet\"\n\n# Define paths for categories\ncategories = {\n    \"non shop lifters\": 0,  # Label 0\n    \"shop lifters\": 1       # Label 1\n}\n\n# Collect video paths and labels\nvideo_paths = []\nlabels = []\n\nfor category, label in categories.items():\n    folder_path = os.path.join(dataset_path, category)\n    unique_videos = get_unique_videos(folder_path, underscore_count=4 if label == 0 else 3)  \n    video_paths.extend(unique_videos)\n    labels.extend([label] * len(unique_videos))  # Assign label to each video\n    \nlabel_counts = Counter(labels)\nmin_class_count = min(label_counts.values())\nstratify = labels if min_class_count >= 2 else None\n\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    video_paths, labels, test_size=0.3,random_state=42, stratify=stratify\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:17.623944Z","iopub.execute_input":"2025-03-21T09:06:17.624196Z","iopub.status.idle":"2025-03-21T09:06:17.643961Z","shell.execute_reply.started":"2025-03-21T09:06:17.624175Z","shell.execute_reply":"2025-03-21T09:06:17.643106Z"}},"outputs":[{"name":"stdout","text":"\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/non shop lifters' before filtering: 531\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/non shop lifters' after filtering: 313\n\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/shop lifters' before filtering: 324\nTotal videos in '/kaggle/input/shoplift/Shop DataSet/shop lifters' after filtering: 324\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Load Model & Feature Extractor\nmodel_name = \"MCG-NJU/VideoMAE-base\"\n\nprocessor = VideoMAEImageProcessor.from_pretrained(model_name)\nmodel = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=2)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:17.645966Z","iopub.execute_input":"2025-03-21T09:06:17.646197Z","iopub.status.idle":"2025-03-21T09:06:22.303451Z","shell.execute_reply.started":"2025-03-21T09:06:17.646176Z","shell.execute_reply":"2025-03-21T09:06:22.302761Z"}},"outputs":[{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/VideoMAE-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"VideoMAEForVideoClassification(\n  (videomae): VideoMAEModel(\n    (embeddings): VideoMAEEmbeddings(\n      (patch_embeddings): VideoMAEPatchEmbeddings(\n        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n      )\n    )\n    (encoder): VideoMAEEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VideoMAELayer(\n          (attention): VideoMAESdpaAttention(\n            (attention): VideoMAESdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): VideoMAESelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): VideoMAEIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VideoMAEOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class VideoDataset(Dataset):\n    def __init__(self, video_paths, labels, processor, num_frames=16):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.processor = processor\n        self.num_frames = num_frames\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n\n        \n        # Extract frames\n        frames = self.extract_frames(video_path, self.num_frames)\n\n        # Preprocess frames\n        inputs = self.processor(frames, return_tensors=\"pt\")\n\n        # Add label\n        inputs[\"labels\"] = torch.tensor(label)\n\n        return inputs\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def extract_frames(self, video_path, num_frames):\n        cap = cv2.VideoCapture(video_path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        frame_indices = np.linspace(0, total_frames-1, num_frames).astype(int)\n\n        frames = []\n        for idx in frame_indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n            ret, frame = cap.read()\n            if ret:\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n                frames.append(frame)\n\n        cap.release()\n        return frames","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:22.304389Z","iopub.execute_input":"2025-03-21T09:06:22.304710Z","iopub.status.idle":"2025-03-21T09:06:22.311045Z","shell.execute_reply.started":"2025-03-21T09:06:22.304651Z","shell.execute_reply":"2025-03-21T09:06:22.310066Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = VideoDataset(train_paths, train_labels, processor)\nval_dataset = VideoDataset(val_paths, val_labels, processor)\n\n# Define DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:22.311829Z","iopub.execute_input":"2025-03-21T09:06:22.312090Z","iopub.status.idle":"2025-03-21T09:06:22.332467Z","shell.execute_reply.started":"2025-03-21T09:06:22.312071Z","shell.execute_reply":"2025-03-21T09:06:22.331715Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=5e-5)\nscheduler = StepLR(optimizer, step_size=5, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:22.333318Z","iopub.execute_input":"2025-03-21T09:06:22.333538Z","iopub.status.idle":"2025-03-21T09:06:22.347526Z","shell.execute_reply.started":"2025-03-21T09:06:22.333517Z","shell.execute_reply":"2025-03-21T09:06:22.346910Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_model(model, optimizer, scheduler, criterion, train_loader, val_loader, num_epochs=10, save_path='/kaggle/working/shoplifting_detector.pt'):\n    best_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss, correct = 0.0, 0\n\n        for batch in train_loader:\n            inputs = batch\n            labels = inputs.pop(\"labels\").to(device).long()\n            pixel_values = inputs[\"pixel_values\"].to(device)\n\n            # Remove the extra dimension (if present)\n            if pixel_values.dim() == 6:\n                pixel_values = pixel_values.squeeze(1)  # Remove the second dimension (1)\n\n            # Ensure pixel_values is in the correct shape\n            # VideoMAE expects (batch_size, num_frames, num_channels, height, width)\n            if pixel_values.dim() != 5:\n                raise ValueError(f\"Unexpected shape for pixel_values: {pixel_values.shape}\")\n\n            optimizer.zero_grad()\n            outputs = model(pixel_values=pixel_values)\n\n            loss = criterion(outputs.logits, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            correct += (outputs.logits.argmax(dim=-1) == labels).sum().item()\n\n        train_loss /= len(train_loader)\n        train_acc = correct / len(train_loader.dataset)\n\n        # Validation loop\n        model.eval()\n        val_loss, val_correct = 0.0, 0\n        with torch.no_grad():\n            for batch in val_loader:\n                inputs = batch\n                labels = inputs.pop(\"labels\").to(device).long()\n                pixel_values = inputs[\"pixel_values\"].to(device)\n\n\n                # Remove the extra dimension (if present)\n                if pixel_values.dim() == 6:\n                    pixel_values = pixel_values.squeeze(1)  # Remove the second dimension (1)\n\n                outputs = model(pixel_values=pixel_values)\n                val_loss += criterion(outputs.logits, labels).item()\n                val_correct += (outputs.logits.argmax(dim=-1) == labels).sum().item()\n\n        val_loss /= len(val_loader)\n        val_acc = val_correct / len(val_loader.dataset)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n        # Save the best model\n        if val_loss < best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), save_path)\n            print(f\"Best model saved with val loss {best_loss:.4f}\")\n\n        # Step the scheduler\n        scheduler.step()\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:22.348278Z","iopub.execute_input":"2025-03-21T09:06:22.348507Z","iopub.status.idle":"2025-03-21T09:06:22.361422Z","shell.execute_reply.started":"2025-03-21T09:06:22.348486Z","shell.execute_reply":"2025-03-21T09:06:22.360755Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"Training Model...\")\ntrained_model = train_model(model, optimizer, scheduler, criterion, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T09:06:22.362081Z","iopub.execute_input":"2025-03-21T09:06:22.362270Z","iopub.status.idle":"2025-03-21T10:40:06.467585Z","shell.execute_reply.started":"2025-03-21T09:06:22.362253Z","shell.execute_reply":"2025-03-21T10:40:06.466545Z"}},"outputs":[{"name":"stdout","text":"Training Model...\nEpoch 1/10, Train Loss: 0.6274, Train Acc: 0.6517, Val Loss: 0.3955, Val Acc: 0.8958\nBest model saved with val loss 0.3955\nEpoch 2/10, Train Loss: 0.2193, Train Acc: 0.9169, Val Loss: 0.3220, Val Acc: 0.8542\nBest model saved with val loss 0.3220\nEpoch 3/10, Train Loss: 0.1147, Train Acc: 0.9618, Val Loss: 0.0362, Val Acc: 0.9948\nBest model saved with val loss 0.0362\nEpoch 4/10, Train Loss: 0.0037, Train Acc: 1.0000, Val Loss: 0.0306, Val Acc: 0.9948\nBest model saved with val loss 0.0306\nEpoch 5/10, Train Loss: 0.0550, Train Acc: 0.9730, Val Loss: 0.0291, Val Acc: 0.9844\nBest model saved with val loss 0.0291\nEpoch 6/10, Train Loss: 0.0064, Train Acc: 1.0000, Val Loss: 0.0140, Val Acc: 0.9948\nBest model saved with val loss 0.0140\nEpoch 7/10, Train Loss: 0.0023, Train Acc: 1.0000, Val Loss: 0.0138, Val Acc: 0.9948\nBest model saved with val loss 0.0138\nEpoch 8/10, Train Loss: 0.0015, Train Acc: 1.0000, Val Loss: 0.0139, Val Acc: 0.9948\nEpoch 9/10, Train Loss: 0.0011, Train Acc: 1.0000, Val Loss: 0.0140, Val Acc: 0.9948\nEpoch 10/10, Train Loss: 0.0009, Train Acc: 1.0000, Val Loss: 0.0146, Val Acc: 0.9948\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}